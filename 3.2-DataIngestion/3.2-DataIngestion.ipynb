{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x22d6f3c16d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"A speech about life can cover a variety of topics, including how to live a meaningful life, how to react to challenges, and how to be grateful for what you have: \\nBe grateful: Life is a gift from God, and you should be grateful for the positive things in your life, like nature, family, and friends. \\nLive in the moment: Life is a continuous process that will end, so you should live each moment to the fullest. \\nBe kind: Kindness is followed by love, so you should always be kind to others. \\nDon't compare yourself to others: You are unique and important, so you shouldn't compare yourself to others. \\nReact to challenges: Challenges are part of life, and you should be ready to face them. \\nDon't take life for granted: You should respect life and cherish what you have. \\nLive a meaningful life: You should live a meaningful life and support others to do the same. \\nBe ready for death: Death is inevitable, but you shouldn't let it discourage you from living life to the fullest. \\n\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents\n",
    "# entire speech.txt load into one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'resume22.pdf', 'page': 0}, page_content=\"Rutuja Barbande\\nrutujabarbande22@email.com\\nPhone no.:9766301079 www.linkedin.com/in/rutujabarbande\\nChatrpati Shambhajinager github.com/78856Rutuja\\nSummary\\nAs an Al and data analysis expert, I specialize in machine learning, natural language processing\\n(NLP), and data interpretation using Python. I have a solid background in python. My goal is to provide\\ninnovative solutions for important projects by applying my technical skills and eagerness to learn and\\ngrow. I'm skilled in machine learning algorithms, neural networks, deep learning, and NLP. I use tools\\nlike Pandas, Numpy, TensorFlow, Keras, PyTorch, and Scikit-Learn. I'm also proficient in data\\nprocessing, model evaluation, and feature engineering.\\nSkills\\n\\uf06c Programming language : Python, Java\\n\\uf06c Frameworks: Pandas, Numpy, TensorFlow, Keras, PyTorch\\n\\uf06c SQL\\n\\uf06c Deep learning\\n\\uf06c Machine learning\\n\\uf06c Data analysis and visualization\\n\\uf06c Natural Language processing\\nExperience\\nCompany Naskraft. Pvt ltd\\nIntern 2023\\nI have done 6 month internship after Engineering at Naskraft private limited , Chatrpati\\nShambhajinager.I have learn skills related to website development and android development.Developed\\nAndroid applications using Java within the Android Studio environment.\\nEducation\\n\\uf06c Center For Development of Advanced Computing, Pune\\nPost Graduate Diploma in Artificial Intelligence |75% 2024\\n\\uf06c Jawaharlal Nehru Engineering college\\nBachelor's Degree in Electronic and Telecommunication CGPA: 8.60 2023\\n\\uf06c MGM College of Polytechnic\\nDiploma in Electronic and Telecommunication|91% 2020\\n\\uf06c A.K. Waghmare High school\\nClass 10th | 80%\"),\n",
       " Document(metadata={'source': 'resume22.pdf', 'page': 1}, page_content='Projects\\n\\uf06c Code-Mixed Text Translation\\nIn this project, we created a translation system that can handle code-mixed text, focusing\\non translating Marathi into English. Code-mixed text combines words and phrases from\\ndifferent languages, making it challenging for traditional translation models. Our system uses\\nadvanced natural language processing (NLP) techniques to accurately detect and translate this\\nmixed language. By focusing on Marathi, which is widely spoken in India, we aim to meet the\\ngrowing need for effective translation tools in multilingual environments. Our goal is to\\ndevelop a reliable and efficient translation system that can be expanded to other Indic\\nlanguages, helping to overcome language barriers and improve communication in diverse\\nlinguistic settings.\\n\\uf06c Face Emotion Recognition\\nDeveloped a real-time emotion detection system utilizing a Convolutional Neural\\nNetwork (CNN) model. The system captures live video feed using a webcam and processes\\nthe input frames to detect facial expressions. It uses the OpenCV library to detect faces and pre\\nprocesses the the image data before feeding it into the trained CNN model. The model classifies\\nemotions into seven categories: angry, disgust, fear, happy, neutral, sad, and surprise. The\\nproject demonstrates a practical application of computer vision and deep learning for real-time\\nemotion analysis, offering potential uses in areas such as human-computer interaction.\\nLanguages known\\n\\uf06c English\\n\\uf06c Marathi\\n\\uf06c Hindi\\nHobbies\\n\\uf06c Swimming\\n\\uf06c Travel')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reding a PDF File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"resume22.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://www.geeksforgeeks.org/introduction-to-generative-pre-trained-transformer-gpt/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=\"article--viewer\")))\n",
    "# class will give specific information from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/introduction-to-generative-pre-trained-transformer-gpt/'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to Generative Pre-trained Transformer (GPT)\\n\\n\\n\\nLast Updated : \\n12 Jul, 2024\\n\\n\\n\\n\\n\\n\\nSummarize\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\nThe Generative Pre-trained Transformer (GPT) is a model, developed by Open AI to understand and generate human-like text. GPT has revolutionized how machines interact with human language, enabling more intuitive and meaningful communication between humans and computers. In this article, we are going to explore more about Generative Pre-trained Transformer. Table of Content\\nWhat is a Generative Pre-trained Transformer?Background and Development of GPT Architecture of Generative Pre-trained TransformerTraining Process of Generative Pre-trained TransformerApplications of Generative Pre-trained TransformerAdvantages of GPTEthical ConsiderationsConclusionWhat is a Generative Pre-trained Transformer?GPT is based on the transformer architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The core idea behind the transformer is the use of self-attention mechanisms that process words in relation to all other words in a sentence, contrary to traditional methods that process words in sequential order. This allows the model to weigh the importance of each word no matter its position in the sentence, leading to a more nuanced understanding of language.As a generative model, GPT can produce new content. When provided with a prompt or a part of a sentence, GPT can generate coherent and contextually relevant continuations. This makes it extremely useful for applications like creating written content, generating creative writing, or even simulating dialogue.Background and Development of GPT The progress of GPT (Generative Pre-trained Transformer) models by OpenAI has been marked by significant advancements in natural language processing. Here’s a chronological overview:GPT (June 2018): The original GPT model was introduced by OpenAI as a pre-trained transformer model that achieved state-of-the-art results on a variety of natural language processing tasks. It featured 12 layers, 768 hidden units, and 12 attention heads, totaling 117 million parameters. This model was pre-trained on a diverse dataset using unsupervised learning and fine-tuned for specific tasks.GPT-2 (February 2019): An upgrade from its predecessor, GPT-2 featured 48 transformer blocks, 1,600 hidden units, and 25 million parameters in its smallest version, up to 1.5 billion parameters in its largest. OpenAI initially delayed the release of the most powerful versions due to concerns about potential misuse. GPT-2 demonstrated an impressive ability to generate coherent and contextually relevant text over extended passages.GPT-3 (June 2020): GPT-3 marked a massive leap in the scale and capability of language models with 175 billion parameters. It improved upon GPT-2 in almost all aspects of performance and demonstrated abilities across a broader array of tasks without task-specific tuning. GPT-3\\'s performance showcased the potential for models to exhibit behaviors resembling understanding and reasoning, igniting widespread discussion about the implications of powerful AI models.GPT-4 (March 2023): GPT-4 expanded further on the capabilities of its predecessors, boasting more nuanced and accurate responses, and improved performance in creative and technical domains. While the exact parameter count has not been officially disclosed, it is understood to be significantly larger than GPT-3 and features architectural improvements that enhance reasoning and contextual understanding.Architecture of Generative Pre-trained TransformerThe transformer architecture, which is the foundation of GPT models, is made up of feedforward neural networks and layers of self-attention processes. Important elements of this architecture consist of:Self-Attention System: This enables the model to evaluate each word\\'s significance within the context of the complete input sequence. It makes it possible for the model to comprehend word linkages and dependencies, which is essential for producing content that is logical and suitable for its context.Layer normalization and residual connections: By reducing problems such as disappearing and exploding gradients, these characteristics aid in training stabilization and enhance network convergence.Feedforward Neural Networks: These networks process the output of the attention mechanism and add another layer of abstraction and learning capability. They are positioned between self-attention layers.Detailed Explanation of the GPT Architecture  Input EmbeddingInput: The raw text input is tokenized into individual tokens (words or subwords).Embedding: Each token is converted into a dense vector representation using an embedding layer.Positional Encoding: Since transformers do not inherently understand the order of tokens, positional encodings are added to the input embeddings to retain the sequence information.Dropout Layer: A dropout layer is applied to the embeddings to prevent overfitting during training. Transformer BlocksLayerNorm: Each transformer block starts with a layer normalization.Multi-Head Self-Attention: The core component, where the input passes through multiple attention heads.Add & Norm: The output of the attention mechanism is added back to the input (residual connection) and normalized again.Feed-Forward Network: A position-wise feed-forward network is applied, typically consisting of two linear transformations with a GeLU activation in between.Dropout: Dropout is applied to the feed-forward network output.Layer Stack: The transformer blocks are stacked to form a deeper model, allowing the network to capture more complex patterns and dependencies in the input.Final LayersLayerNorm: A final layer normalization is applied.Linear: The output is passed through a linear layer to map it to the vocabulary size.Softmax: A softmax layer is applied to produce the final probabilities for each token in the vocabulary.Training Process of Generative Pre-trained TransformerLarge-scale text data corpora are used for unsupervised learning to train GPT algorithms. There are two primary stages to the training:Pre-training: Known as language modeling, this stage teaches the model to anticipate the word that will come next in a sentence. In order to make that the model can produce writing that is human-like in a variety of settings and domains, this phase makes use of a wide variety of internet material.Fine-tuning: While GPT models perform well in zero-shot and few-shot learning, fine-tuning is occasionally necessary for particular applications. In order to improve the model\\'s performance, this entails training it on data specific to a given domain or task.Applications of Generative Pre-trained TransformerThe versatility of GPT models allows for a wide range of applications, including but not limited to:Content Creation: GPT can generate articles, stories, and poetry, assisting writers with creative tasks.Customer Support: Automated chatbots and virtual assistants powered by GPT provide efficient and human-like customer service interactions.Education: GPT models can create personalized tutoring systems, generate educational content, and assist with language learning.Programming: GPT-3\\'s ability to generate code from natural language descriptions aids developers in software development and debugging.Healthcare: Applications include generating medical reports, assisting in research by summarizing scientific literature, and providing conversational agents for patient support.Advantages of GPTFlexibility: GPT\\'s architecture allows it to perform a wide range of language-based tasks.Scalability: As more data is fed into the model, its ability to understand and generate language improves.Contextual Understanding: Its deep learning capabilities allow it to understand and generate text with a high degree of relevance and contextuality.Ethical ConsiderationsDespite their powerful capabilities, GPT models raise several ethical concerns:Bias and Fairness: GPT models can inadvertently perpetuate biases present in the training data, leading to biased outputs.Misinformation: The ability to generate coherent and plausible text can be misused to spread false information.Job Displacement: Automation of tasks traditionally performed by humans could lead to job losses in certain sectors.OpenAI addresses these concerns by implementing safety measures, encouraging responsible use, and actively researching ways to mitigate potential harms.ConclusionArtificial intelligence has advanced significantly with the Generative Pre-trained Transformer models, especially in natural language processing. Every version of GPT, from GPT-1 to GPT-4, has increased the capabilities of AI in terms of comprehending and producing human language. Although GPT models\\' capabilities present a plethora of prospects in a variety of sectors, it is imperative to tackle the ethical issues that come with them in order to guarantee their responsible and advantageous application. GPT models are expected to stay at the vanguard of AI technology evolution, propelling innovation and industry revolution.\\n\\n\\n\\n\\n\\n\\n\\n\\nS\\n\\n\\n\\n\\n \\n\\nalka1974 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n  \\n\\nNext Article\\n\\n\\n\\n\\nDeep Transfer Learning - Introduction\\n\\n\\n\\n\\n\\n\\nSimilar Reads\\n\\n\\n\\nPre-Trained Word Embedding in NLP\\nWord Embedding is an important term in Natural Language Processing and a significant breakthrough in deep learning that solved many problems. In this article, we\\'ll be looking into what pre-trained word embeddings in NLP are. Table of ContentWord EmbeddingsChallenges in building word embedding from scratchPre Trained Word EmbeddingsWord2Vec GloVeBE\\n\\n\\n\\n9 min read\\n\\n\\n\\n\\nPre-trained Word embedding using Glove in NLP models\\nIn this article, we are going to see Pre-trained Word embedding using Glove in NLP models using Python. What is GloVe?Global Vectors for Word Representation, or GloVe for short, is an unsupervised learning algorithm that generates vector representations, or embeddings, of words. Researchers Richard Socher, Christopher D. Manning, and Jeffrey Pennin\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\nTop Pre-Trained Models for Image Classification\\nPre-trained models are neural networks trained on large datasets before being fine-tuned for specific tasks. These models capture intricate patterns and features, making them highly effective for image classification. By leveraging pre-trained models, developers can save time and computational resources. They can also achieve high accuracy with les\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\n\\nBuilding a Simple Language Translation Tool Using a Pre-Trained Translation Model\\nTranslation is common among the applications of Natural Language Processing and machine learning. Though due to advancements in technology mainly in the pre-trained models, and transformers, it becomes easier to create a suitable language translation tool. Here in this article, we will create the language translation application from the pre-traine\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\nIs Auto-GPT Worth Using Without GPT-4?\\nIn today’s digital landscape, AI chatbots like chatGPT are used for diverse purposes. A bot can effectively qualify leads, drive large sales pipelines, and interact with customers. These chatbots have become essential for businesses as they help get contacts, gather information, suggest items, and facilitate the customer onboarding process without\\n\\n\\n\\n8 min read\\n\\n\\n\\n\\nLLM vs GPT : Comparing Large Language Models and GPT\\nIn recent years, the field of natural language processing (NLP) has made tremendous strides, largely due to the development of large language models (LLMs) and, more specifically, the Generative Pre-trained Transformer (GPT) series. Both LLMs and GPTs have transformed how machines understand and generate human language. Table of Content What is a L\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nFrom GPT-3 to GPT-4: Evolution and Innovations in Large Language Models\\nThe progression of artificial intelligence (AI) in recent years has been nothing short of extraordinary, with significant strides particularly evident in the realm of natural language processing (NLP). Central to this advancement is the development of large language models (LLMs) like OpenAI\\'s GPT series. This article explores the evolution from GP\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\n\\nThe Evolution of Language Models: From GPT-1 to GPT-4 and Beyond\\nThe field of Natural Language Processing (NLP) has seen dramatic advancements over the past few years, largely driven by the evolution of language models. From the inception of GPT-1 to the revolutionary capabilities of GPT-4, and with the future promising even more breakthroughs, understanding this progression sheds light on how these models have\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nGPT 4 vs GPT 3: Top Differences That You Should Know in 2024\\nThe AI chatbot is completely changed by ChatGPT. It has seized the AI market and established a powerful foothold there. Now that the ChatGPT has figured things out, it is prepared to advance with new plans and advancements. A new version of ChatGPT was thus released. An artificial intelligence model called GPT-4 has begun to be released by OpenAI a\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nMeet DarkBERT: New AI Trained on the Dark Web\\nThe Dark Web getting more and more dangerous many report says that in recent years after the development of AI dark web has also been AI for cybercrime. Now, to protect from cybercrime a language model specifically trained on the shadowy underbelly of the internet, the dark web this AI module known as DarkBERT, In a daring exploration of the hidden\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\nShould a Model Be Re-Trained If New Observations Are Available?\\nAnswer: Yes, re-training the model with new observations can improve its performance and adapt it to the evolving data distribution.When new observations become available, re-training the model can be beneficial for several reasons: Adaptation to Evolving Data Distribution: As the underlying data distribution may change over time due to various fac\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nHow to Create a 2D Partial Dependence Plot on a Trained Random Forest Model in R\\nRandom Forest, a powerful ensemble learning algorithm, is widely used for regression and classification tasks due to its robustness and ability to handle complex data. However, understanding how individual features influence the model\\'s predictions can be challenging. Partial Dependence Plots (PDPs) provide a valuable tool for visualizing the relat\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nR - Calculate Test MSE given a trained model from a training set and a test set\\nMean Squared Error (MSE) is a widely used metric for evaluating the performance of regression models. It measures the average of the squares of the errors. the average squared difference between the actual and predicted values. The Test MSE, specifically, helps in assessing how well the model generalizes to new, unseen data. In this article, we wil\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nIntroduction to NExT-GPT: Any-to-Any Multimodal LLM\\nThe field of artificial intelligence (AI) has seen rapid advancements in recent years, particularly in the development of large language models (LLMs) like GPT-4. These models have primarily focused on text-based tasks, excelling in natural language understanding and generation. However, as multimodal applications—those that involve various forms o\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nText to text Transfer Transformer  in Data Augmentation\\nDo you want to achieve \\'the-state-of-the-art\\' results in your next NLP project?Is your data insufficient for training the machine learning model?Do you want to improve the accuracy of your machine learning model with some extra data? If yes, all you need is Data Augmentation. Whether you are building text classification, summarization, question ans\\n\\n\\n\\n8 min read\\n\\n\\n\\n\\nSparse Transformer:  Stride and Fixed Factorized Attention\\nStrided and Fixed attention were proposed by researchers @ OpenAI in the paper called \\'Generating Long Sequences with Sparse Transformers \\'. They argue that Transformer is a powerful architecture, However, it has the quadratic computational time and space w.r.t the sequence length. So, this inhibits the ability to use large sequences. That\\'s why th\\n\\n\\n\\n8 min read\\n\\n\\n\\n\\nTransformer Neural Network In Deep Learning - Overview\\nIn this article, we are going to learn about Transformers. We\\'ll start by having an overview of Deep Learning and its implementation. Moving ahead, we shall see how Sequential Data can be processed using Deep Learning and the improvement that we have seen in the models over the years. Deep Learning So now what exactly is Deep Learning? But before w\\n\\n\\n\\n10 min read\\n\\n\\n\\n\\nAudio Transformer\\nFrom revolutionizing computer vision to advancing natural language processing, the realm of artificial intelligence has ventured into countless domains. Yet, there\\'s one realm that\\'s been a consistent source of both fascination and complexity: audio. In the age of voice assistants, automatic speech recognition, and immersive audio experiences, the\\n\\n\\n\\n15+ min read\\n\\n\\n\\n\\nFNet: A Transformer Without Attention Layer\\nThis article delves into FNet, a transformative architecture that reimagines the traditional transformer by discarding attention mechanisms entirely. Let\\'s begin the journey to explore FNet, but first, let\\'s look at the limitations of transformers. What is FNet?In contrast to conventional transformer architectures, like the popular Transformer mode\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\nTransformer XL: Beyond a Fixed-Length Context\\nTransformer XL is short for Transformer Extra Long. The Transformer-XL model was introduced in the paper titled \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,\" authored by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Natural Language Processing has experienced significant pr\\n\\n\\n\\n8 min read\\n\\n\\n\\n\\nSentence Similarity using BERT Transformer\\nConventional techniques for assessing sentence similarity frequently struggle to grasp the intricate nuances and semantic connections found within sentences. With the rise of Transformer-based models such as BERT, RoBERTa, and GPT, there is potential to improve sentence similarity measurements with increased accuracy and contextual awareness. The a\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\nQuantile Transformer for Outlier Detection\\nData transformation is a mathematical function that changes the data into a scaled value, which makes it possible to compare different columns, e.g., salary in INR with weight in kilograms. Transforming the data will satisfy certain mathematical assumptions such as normalization, standardization, homogeneity, linearity, etc. Quantile Transformer is\\n\\n\\n\\n11 min read\\n\\n\\n\\n\\nTransformer Attention Mechanism in NLP\\nThe Transformer model, introduced in the groundbreaking paper \"Attention is All You Need\" by Vaswani et al., has revolutionized the field of natural language processing (NLP). Central to the Transformer\\'s success is the attention mechanism, which allows the model to weigh the importance of different words in a sentence, regardless of their position\\n\\n\\n\\n10 min read\\n\\n\\n\\n\\nTransformer Model from Scratch using TensorFlow\\nTransformers are a deep learning architecture designed for sequence-to-sequence tasks. Unlike traditional sequence models such as recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), transformers rely entirely on a mechanism known as self-attention to draw global dependencies between input and output. In this guide, we will\\n\\n\\n\\n10 min read\\n\\n\\n\\n\\nMachine Translation with Transformer in Python\\nMachine translation converts a sequence of text from one language to another. Popular online translation services like Google Translate, Microsoft Translator, and others use machine translation techniques to provide users with quick and accessible translations between a wide range of languages. Transformer models are the most recent and widely adop\\n\\n\\n\\n11 min read\\n\\n\\n\\n\\nGAN vs. Transformer Models\\nGenerative models have gained immense popularity in the realm of machine learning due to their ability to generate data, whether it’s realistic images, coherent text, or plausible audio. Among the most renowned architectures are Generative Adversarial Networks (GANs) and Transformer models. Each of these models has proven its effectiveness in speci\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nHow to Use the Hugging Face Transformer Library for Sentiment Analysis\\nThe Hugging Face Transformer library is now a popular choice for developers working on Natural Language Processing (NLP) projects. It simplifies access to a range of pretrained models like BERT, GPT, and RoBERTa, making it easier for developers to utilize advanced models without extensive knowledge in deep learning. The Transformer library enables\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\nBuilding a Vision Transformer from Scratch in PyTorch\\nVision Transformers (ViTs) have revolutionized the field of computer vision by leveraging transformer architecture, which was originally designed for natural language processing. Unlike traditional CNNs, ViTs divide an image into patches and treat them as tokens, allowing the model to learn spatial relationships effectively. In this tutorial, we’ll\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\nVision Transformer (ViT) Architecture\\nVision Transformer (ViT) is an innovative deep learning architecture designed to process visual data using the same transformer architecture that revolutionized natural language processing (NLP). Unlike convolutional neural networks (CNNs), which rely on convolutions to capture local spatial features, Vision Transformers adopt the self-attention me\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\nOpen AI GPT-3\\nOpen AI GPT-3 is proposed by the researchers at OpenAI as a next model series of GPT models in the paper titled \"Language Models are few shots learners\". It is trained on 175 billion parameters, which is 10x more than any previous non-sparse model. It can perform various tasks from machine translation to code generation etc. The model is not availa\\n\\n\\n\\n11 min read\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nAI-ML-DS\\n\\n\\nBlogathon\\n\\n\\nArtificial Intelligence\\n\\n\\nData Science Blogathon 2024\\n\\n\\nGenerative AI\\n \\n+1 More\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install arxiv\n",
    "#!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs=ArxivLoader(query=\"1706.03762\",load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2021-03-29', 'Title': '\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest Value for us?', 'Authors': 'Bin Liu', 'Summary': 'AI has surpassed humans across a variety of tasks such as image\\nclassification, playing games (e.g., go, \"Starcraft\" and poker), and protein\\nstructure prediction. However, at the same time, AI is also bearing serious\\ncontroversies. Many researchers argue that little substantial progress has been\\nmade for AI in recent decades. In this paper, the author (1) explains why\\ncontroversies about AI exist; (2) discriminates two paradigms of AI research,\\ntermed \"weak AI\" and \"strong AI\" (a.k.a. artificial general intelligence); (3)\\nclarifies how to judge which paradigm a research work should be classified\\ninto; (4) discusses what is the greatest value of \"weak AI\" if it has no chance\\nto develop into \"strong AI\".'}, page_content='arXiv:2103.15294v1  [cs.AI]  29 Mar 2021\\n1\\n“Weak AI” is Likely to Never Become “Strong AI”,\\nSo What is its Greatest Value for us?\\n⋆Bin Liu\\nFirst posted March 30th, 2021\\nAbstract\\nAI has surpassed humans across a variety of tasks such as image classiﬁcation, playing games (e.g., go,\\n“Starcraft” and poker), and protein structure prediction. However, at the same time, AI is also bearing serious\\ncontroversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In\\nthis paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research,\\ntermed “weak AI” and “strong AI” (a.k.a. artiﬁcial general intelligence); (3) clariﬁes how to judge which paradigm\\na research work should be classiﬁed into; (4) discusses what is the greatest value of “weak AI” if it has no chance\\nto develop into “strong AI”.\\nIndex Terms\\nArtiﬁcial intelligence, artiﬁcial general intelligence, deep learning, weak AI, strong AI\\nI. INTRODUCTION\\nThe last decade has seen impressive applications of AI represented mostly by deep neural networks,\\ni.e., deep learning [1]. The striking point lies in that the computing agent has reached and even surpassed\\nhumans in many tasks, e.g., image classiﬁcation [2], speech recognition [3, 4], games [5–7], protein\\nstructure prediction [8]. Even ten years ago, it was hard to imagine that AI would achieve so many\\namazing breakthroughs.\\nOn the other side, AI is also bearing serious controversies during the same period. Among the critics,\\nJudea Pearl, a pioneer for probabilistic reasoning in AI and a winner of the Turing award, argues that\\n“... all the impressive achievements of deep learning amount to just curve ﬁtting,” and a necessary ability\\nto be supplemented for AI is causal reasoning [9, 10]. Gary Marcus, a professor of cognitive science,\\n⋆B. Liu is with Zhejiang Lab, Hangzhou, China. e-mail: bins@ieee.org or liubin@zhejianglab.com.\\n2\\nsummarizes ten limitations of deep learning [11], namely, “... it is data-hungry, ... it has limited capacity\\nfor transfer, ... it has no natural way to deal with hierarchical structure, ... it struggles with open-ended\\ninference, ... it is not sufﬁciently transparent, ... it has not been well integrated with prior knowledge,\\n... it cannot inherently distinguish causation from correlation, ... it presumes a largely stable world, in\\nways that may be problematic, ... it works well as an approximation, but its answers often cannot be fully\\ntrusted, ... it is difﬁcult to engineer with”. In a recent issue of the journal Frontiers in Psychology, another\\ncognitive scientist J. Mark Bishop argues that AI “is stupid and causal reasoning will not ﬁx it” [12].\\nIn this paper, I attempt to concisely respond to current controversies about AI. Speciﬁcally, I emphasize\\ndiscrimination between two paradigms of AI research, namely “weak AI” and “strong AI” (Section II);\\nprovide a conceptual guide to judge which paradigm a research work should be classiﬁed into (Section\\nII-A), explain why controversies about AI last (Section III), present major views on whether “weak AI”\\nwill grow into “strong AI” (Section IV) and discuss what is the greatest value of “weak AI” if it has no\\nchance to become “strong AI” (Section V).\\nII. WHAT DO “WEAK AI” AND “STRONG AI” MEAN?\\n“Weak AI” and “Strong AI” are two terms coined by John Searle in the “Chinese room argument”\\n(CRA) [13]. CRA is a thought experiment as follows: “Searle imagines himself alone in a room following\\na computer program for responding to Chinese characters slipped under the door. Searle understands\\nnothing of Chinese, and yet, by following the program for manipulating symbols and numerals just as\\na computer does, he sends appropriate strings of Chinese characters back out under the door, and this\\nleads those outside to mistakenly suppose there is a Chinese speaker in the room” [14]. The term “strong\\nAI” entails that, “... the computer is not merely a tool in the study of the mind; rather, the appropriately\\nprogrammed computer really is a mind, in the sense that computers given the right programs can be\\nliterally said to understand and have other cognitive states.” In contrast, the term “weak AI” implies that\\n“... the principal value of the computer in the study of the mind is that it gives us a very powerful tool.” J.\\nMark Bishop summarizes that ‘‘weak AI focuses on epistemic issues relating to engineering a simulation\\nof human intelligent behavior, whereas strong AI, in seeking to engineer a computational system with all\\nthe causal power of a mind, focuses on the ontological ” [12].\\nI borrow the terms “weak AI” and “strong AI” here without an intent to discuss CRA. See related\\ndiscussions in e.g., [15–18].\\n3\\nSimply put, “weak AI” represents computational systems that exhibit as if they own human intelligence,\\nbut they do not. In contrast, “strong AI” represents computational systems that have human intelligence.\\nCorrespondingly, all AI research can be categorized into two paradigms: one is targeted for realizing\\n“strong AI”; and the other produces advanced “weak AI” systems to meet a variety of practical needs.\\nA. How to Judge a Research Work Belongs to Which Paradigm?\\nThe biggest motivation for realizing “strong AI” is to answer the question: what are the generation\\nmechanisms of humans’ intelligence and how to implement these mechanisms with a machine. Therefore,\\ngiven a research work, it is easy to judge whether it belongs to the “strong AI” paradigm. If this work\\nprovides any new and useful clue for us to answer the above question, it falls within the “strong AI”\\nparadigm; otherwise, it belongs to the “weak AI” paradigm.\\nBased on the above method, part of the (especially early) works on neural networks that deepen our\\nunderstanding of the working mechanism of biological neural systems, surely belongs to the “strong AI”\\nparadigm. On the other hand, most research works that involve artiﬁcial neural networks and deep learning,\\neven if they are proposed under the inspiration of research on neuroscience, cognitive science, behavior\\npsychology, they belong to the “weak AI” paradigm as long as they do not give us any new insight on\\nthe generation mechanisms of humans’ intelligence or on how to better implement mechanisms that have\\nalready been found.\\nIII. WHY CONTROVERSIES ABOUT AI LAST?\\nIn controversies about AI, party A believes that AI has made substantial progress in the past decade;\\nparty B doubts or even negates the development of AI.\\nI argue that controversies arise mainly because these two parties mix two different concepts, “weak\\nAI” and “strong AI”, together, when they talk about AI. The fact is that “weak AI” has made substantial\\nprogress in the past decade, while “strong AI” has not. Party A thinks that “weak AI” is an important\\nmember of the AI family; progress gained from “weak AI” also belongs to this AI family. In contrast,\\nin the mind of Party B, there always exists one ideal form of AI, namely a realized “strong AI”, and\\nthe “distance” between current AI and this ideal AI is treated as a criterion for evaluating current AI.\\nCompared with decades ago, current AI still lacks basic human-level abilities such as causal reasoning\\n[9], robust decision making [19], commonsense utilization [20], and knowledge transfer, which implies\\n4\\nthat the “distance” between the realized AI and the ideal “strong AI” has not been remarkably shortened.\\nTherefore, it is reasonable for party B to doubt or even negate the development of AI.\\nA natural question arises: how breakthroughs of “weak AI” have come out in the past decade? Judea\\nPearl argues that “... all the impressive achievements of deep learning amount to just curve ﬁtting”.\\nHowever, the point is that, different from previous ﬁtting methods, deep learning permits to do an\\nextraordinary ﬁtting - ﬁtting multi-modal big data in an end-to-end way. This deep learning type of\\nﬁtting requires a big consumption of both computing and storage resources but avoids labor-intensive\\nfeature engineering. Big data, big computing, and big storage are three requisites that make deep learning\\nsurpass humans in playing Go, image classiﬁcation, speech recognition, and so on. The luck for deep\\nlearning is that the past decade happens to witness great improvements in sensing technologies, wireless\\nmobile phones, cloud computing, computing devices, computer storage, and databases, which give birth\\nto big data, big computing, and big storage required by deep learning.\\nIV. WILL “WEAK AI” GROW INTO “STRONG AI”?\\nA metaphor is often used to reply to this question: the relationship between “weak AI” and “strong AI”\\nis like that between ﬂying machines and birds. Flying machines are not developed by accurately mimicking\\nbirds’ ﬂying. Birds perform much better in maneuvering than the most advanced ﬂying machine today.\\nBirds can ﬂexibly re-purpose their behaviors while ﬂying machines cannot. But the appearance of ﬂying\\nmachines has met demands of speedy transportation and others. People may think that, since it is unlikely\\nand not necessary for ﬂying machines to develop into birds, then similarly, “weak AI” is unlikely and not\\nnecessary to grow into “strong AI”.\\nTo formally consider whether “weak AI” will grow into “strong AI”, let recall the Turing test [21] and the\\nCRA (mentioned in Section II). An example statement of the Turing test is as follows [22]: “Originally\\nknown as the Imitation Game, the test evaluates if a machine’s behavior can be distinguished from a\\nhuman. In this test, there is a person known as the “interrogator” who seeks to identify a difference between\\ncomputer-generated output and human-generated ones through a series of questions. If the interrogator\\ncannot reliably discern the machines from human subjects, the machine passes the test. However, if\\nthe evaluator can identify the human responses correctly, then this eliminates the machine from being\\ncategorized as intelligent.” Through the lens of CRA, Searle argues that the Turing test has serious ﬂaws,\\nas passing the test does not indicate that the machine has consciousness or understanding. The absence\\nof an effective evaluation method hampers the development of “strong AI”.\\n5\\nBesides, philosophers and cognitive scientists often use G¨odel’s ﬁrst incompleteness theorem [23] to\\nargue that a machine cannot generate humans’ consciousness or understanding. See related discussions in\\ne.g., [12].\\nV. WHAT IS THE GREATEST VALUE OF “WEAK AI” FOR US?\\nIn his most recent paper, Geoffrey Hinton states that “The difference between science and philosophy is\\nthat experiments can show that extremely plausible ideas are just wrong and extremely implausible ones,\\nlike learning an entire complicated system by end-to-end gradient decent, are just right” [24]. In [25], Judea\\nPearl argues that “Modern connectionism has in fact been viewed as a Triumph of Radical Empiricism\\nover its rationalistic rivals. Indeed, the ability to emulate knowledge acquisition processes on digital\\nmachines offer enormously ﬂexible testing grounds in which philosophical theories about the balance\\nbetween empiricism and innateness can be submitted to experimental evaluation on digital machines.”\\nCombining their arguments, one can see that they both attribute recent deep learning’s success as a success\\nof empiricism which is data-driven, other than driven by philosophical theory or intuition.\\nA very important lesson that can be learned from the fast-pacing development and applications of AI\\nin the past decade is that deep learning running on big enough data can produce unexpected shortcuts\\nto solve extremely difﬁcult problems. For example, by combining deep learning, reinforcement learning\\n[26], and Monte Carlo tree search [27], a computer program AlphaGO [28] can win the human champion\\nwithout having to understand any of the Go-playing strategies that have been accumulated by humans\\nfor more than four thousand years. The Generative Pre-trained Transformer 3 (GPT-3) [29] can generate\\nhuman-like texts through deep learning without having to understand any syntax or semantics underlying\\nthe texts. It is shown that the greatest value of “weak AI” represented by deep learning lies in that it\\nprovides scalable, less-labor-involved, accurate, and generalizable tools for distilling, representing and\\nthen exploiting patterns hidden from big data. Although such “weak AI” has no real intelligence, to a\\nlarge extent it meets urgent needs for scalable, efﬁcient, and accurate processing of big data.\\nIn a foreseeable future, “weak AI” is likely to become more robustly (with e.g., portfolio [19] or\\ndynamic portfolio methods [30–35]), while a big challenge is how to model “unknown unknowns”; it will\\nperform more automatically through e.g., auto machine learning [36], but it can not become completely\\nautomatic provided that “strong AI” is realized [37]; it may perform as if it owns abilities of cognition\\nand understanding, but it does not.\\n6\\nVI. CONCLUSIONS\\nAI has made great progress in the past decade. It has inﬂuenced almost all facets of human society\\nby providing more efﬁcient algorithmic solutions to representation, management, analysis of multi-modal\\nbig data. Controversies about AI last mainly because “weak AI” becomes so strong while “strong AI” is\\nalmost as weak as it was decades ago. Almost all breakthroughs of AI that have attracted the public’s\\nattention in the past decade are within the “weak AI” paradigm. “Weak AI” is developing much faster\\nthan expected. Even ten years ago, one could not imagine that a computer program would beat the human\\nchampion soon in playing Go. In contrast, the “fruits” people have got from the “strong AI” paradigm are\\nnot so striking as from “weak AI”. I suggest, when talking about AI in the future, one should better make\\na statement in advance whether this talk is about “weak AI” or “strong AI”. In this way, more focused\\nand constructive discussions can be expected.\\nIn a foreseeable future, “weak AI” cannot develop into “strong AI” (see why in Section IV), but it\\nprovides a channel to synthesize advances obtained from related disciplines such as cloud computing,\\ncomputer storage, high-speed wireless mobile communications. Through this synthesis of technologies,\\nmore advanced algorithmic tools will be developed in the “weak AI” paradigm, then “weak AI” will\\ncontinue to inﬂuence human society more profoundly, through big data. The man-computer symbiosis\\nworld that Licklider predicted more than sixty years ago [38] is becoming a reality.\\nREFERENCES\\n[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015.\\n[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” Advances in neural\\ninformation processing systems, vol. 25, pp. 1097–1105, 2012.\\n[3] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke, “The microsoft 2017 conversational speech recognition system,” in\\nIEEE int’l conf. on acoustics, speech and signal processing (ICASSP).\\nIEEE, 2018, pp. 5934–5938.\\n[4] G. Saon, G. Kurata, T. Sercu et al., “English conversational telephone speech recognition by humans and machines,” arXiv preprint\\narXiv:1703.02136, 2017.\\n[5] D. Silver, T. Hubert, J. Schrittwieser et al., “A general reinforcement learning algorithm that masters chess, shogi, and go through\\nself-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.\\n[6] N. Brown and T. Sandholm, “Superhuman ai for heads-up no-limit poker: Libratus beats top professionals,” Science, vol. 359, no.\\n6374, pp. 418–424, 2018.\\n[7] O. Vinyals, I. Babuschkin, W. M. Czarnecki et al., “Grandmaster level in starcraft ii using multi-agent reinforcement learning,” Nature,\\nvol. 575, no. 7782, pp. 350–354, 2019.\\n[8] A. W. Senior, R. Evans, J. Jumper et al., “Improved protein structure prediction using potentials from deep learning,” Nature, vol. 577,\\nno. 7792, pp. 706–710, 2020.\\n7\\n[9] J. Pearl, “The limitations of opaque learning machines,” Possible minds: twenty-ﬁve ways of looking at AI, pp. 13–19, 2019.\\n[10] J. Pearl and D. Mackenzie, “Ai can’t reason why,” Wall Street Journal, 2018.\\n[11] G. Marcus, “Deep learning: A critical appraisal,” arXiv preprint arXiv:1801.00631, 2018.\\n[12] J. M. Bishop, “Artiﬁcial intelligence is stupid and causal reasoning will not ﬁx it,” Frontiers in Psychology, vol. 11, pp. 1–18, 2021.\\n[13] S. John, “Minds, brains, and programs,” Behavioral and Brain Sciences, vol. 3, no. 3, pp. 417–457, 1980.\\n[14] D. Cole, “The chinese room argument,” https://plato.stanford.edu/entries/chinese-room/.\\n[15] G. Rey, “What’s really going on in Searle’s “Chinese room”,” Philosophical Studies, vol. 50, no. 2, pp. 169–85, 1986.\\n[16] M. J. Shaffer, “A logical hole in the chinese room,” Minds and Machines, vol. 19, no. 2, pp. 229–235, 2009.\\n[17] A. Sloman and M. Croucher, “How to turn an information processor into an understander,” Behavioral and Brain Sciences, vol. 3,\\nno. 3, pp. 447–448, 1980.\\n[18] M. A. Boden, Computer models of mind: Computational approaches in theoretical psychology.\\nCambridge University Press, 1988.\\n[19] T. G. Dietterich, “Steps toward robust artiﬁcial intelligence,” AI Magazine, vol. 38, no. 3, pp. 3–24, 2017.\\n[20] G. Marcus, “The next decade in AI: four steps towards robust artiﬁcial intelligence,” arXiv preprint arXiv:2002.06177, 2020.\\n[21] A. M. Turing, “Computing machinery and intelligence,” in Parsing the turing test.\\nSpringer, 2009, pp. 23–65.\\n[22] IBM Cloud Education, “Strong AI,” https://www.ibm.com/cloud/learn/strong-ai.\\n[23] P. Raatikainen, “G¨odel’s incompleteness theorems,” https://plato.stanford.edu/entries/goedel-incompleteness/.\\n[24] G. Hinton, “How to represent part-whole hierarchies in a neural network,” arXiv preprint arXiv:2102.12627, 2021.\\n[25] J. Pearl, “Radical empiricism and machine learning research,” Causal Analysis in Theory and Practice (Blog), vol. 26, 2020.\\n[26] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\\nMIT press, 2018.\\n[27] S. Gelly and D. Silver, “Monte-carlo tree search and rapid action value estimation in computer go,” Artiﬁcial Intelligence, vol. 175,\\nno. 11, pp. 1856–1875, 2011.\\n[28] D. Silver, A. Huang, C. J. Maddison et al., “Mastering the game of go with deep neural networks and tree search,” Nature, vol. 529,\\nno. 7587, pp. 484–489, 2016.\\n[29] T. B. Brown, B. Mann, N. Ryder et al., “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\\n[30] B. Liu, Y. Qi, and K. Chen, “Sequential online prediction in the presence of outliers and change points: an instant temporal structure\\nlearning approach,” Neurocomputing, vol. 413, pp. 240–258, 2020.\\n[31] Y. Qi, B. Liu, Y. Wang, and G. Pan, “Dynamic ensemble modeling approach to nonstationary neural decoding in brain-computer\\ninterfaces,” Advances in neural information processing systems, pp. 6087–6096, 2019.\\n[32] B. Liu, “Robust particle ﬁlter by dynamic averaging of multiple noise models,” in IEEE Int’l Conf. on Acoustics, Speech and Signal\\nProcessing (ICASSP).\\nIEEE, 2017, pp. 4034–4038.\\n[33] Y. Dai and B. Liu, “Robust video object tracking via bayesian model averaging-based feature fusion,” Optical Engineering, vol. 55,\\nno. 8, pp. 1–11, 2016.\\n[34] B. Liu, “Data-driven model set design for model averaged particle ﬁlter,” in IEEE Int’l Conf. on Acoustics, Speech and Signal Processing\\n(ICASSP).\\nIEEE, 2020, pp. 5835–5839.\\n[35] ——, “Instantaneous frequency tracking under model uncertainty via dynamic model averaging and particle ﬁltering,” IEEE Trans. on\\nWireless Communications, vol. 10, no. 6, pp. 1810–1819, 2011.\\n[36] F. Hutter, L. Kotthoff, and J. Vanschoren, Automated machine learning: methods, systems, challenges.\\nSpringer Nature, 2019.\\n[37] B. Liu, “A very brief and critical discussion on automl,” arXiv preprint arXiv:1811.03822, 2018.\\n[38] J. Licklider, “Man-computer symbiosis,” IRE Transactions on human factors in electronics, no. 1, pp. 4–11, 1960.\\n'), Document(metadata={'Published': '2024-02-08', 'Title': 'A Bibliometric View of AI Ethics Development', 'Authors': 'Di Kevin Gao, Andrew Haverly, Sudip Mittal, Jingdao Chen', 'Summary': 'Artificial Intelligence (AI) Ethics is a nascent yet critical research field.\\nRecent developments in generative AI and foundational models necessitate a\\nrenewed look at the problem of AI Ethics. In this study, we perform a\\nbibliometric analysis of AI Ethics literature for the last 20 years based on\\nkeyword search. Our study reveals a three-phase development in AI Ethics,\\nnamely an incubation phase, making AI human-like machines phase, and making AI\\nhuman-centric machines phase. We conjecture that the next phase of AI ethics is\\nlikely to focus on making AI more machine-like as AI matches or surpasses\\nhumans intellectually, a term we coin as \"machine-like human\".'}, page_content=' \\nA bibliometric view of AI Ethics development \\n \\n \\nDi Kevin Gao \\nManagement Department -  \\nCalifornia State University, East \\nBay  \\nHayward, CA, USA \\nkevin.gao@csueastbay.edu \\n \\n  \\nAndrew Haverly \\nComputer Science and \\nEngineering Dept \\nMississippi State University \\nMississippi State, MS 39762 \\narh876@msstate.edu \\n \\n \\nDr. Sudip Mittal  \\nComputer Science and \\nEngineering Dept \\nMississippi State University \\nMississippi State, MS 39762 \\nmittal@cse.msstate.edu \\n \\n \\nDr. Jingdao Chen \\nComputer Science and \\nEngineering Dept \\nMississippi State University \\nMississippi State, MS 39762 \\nchenjingdao@cse.msstate.edu\\n \\n \\n \\nAbstract— Artificial Intelligence (AI) Ethics is a nascent yet \\ncritical research field. Recent developments in generative AI and \\nfoundational models necessitate a renewed look at the problem \\nof AI Ethics. In this study, we perform a bibliometric analysis of \\nAI Ethics literature for the last 20 years based on keyword \\nsearch. Our study reveals a three-phase development in AI \\nEthics, namely an incubation phase, making AI human-like \\nmachines phase, and making AI human-centric machines phase. \\nWe conjecture that the next phase of AI ethics is likely to focus \\non making AI more machine-like as AI matches or surpasses \\nhumans intellectually, a term we coin as “machine-like human”.  \\nKeywords— artificial intelligence ethics, AI ethics, machine \\nethics, algorithm ethics, Roboethics, human-like machine, \\nmachine-like human  \\nI. INTRODUCTION \\nArtificial Intelligence (AI) Ethics is the study of the \\nethical and responsible development and deployment of AI \\ntechnology. Our bibliometric analysis of AI Ethics literature \\npublished between 2004 and 2023 points us to a three-phase \\ndevelopment: 1. Incubation; 2. Making AI human-like \\nmachines; 3. Making AI human-centric machines.  \\nThis article contributes to AI Ethics discussions with \\nunique insights based on keyword usage patterns. It also \\ncontrasts “human-like machine”, “human-centric machine”, and \\n“machine-like human”, which represent the past, current, and \\npotential future phases of AI Ethics development. \\nII. DEFINITIONS AND HISTORICAL DEVELOPMENT  \\nAI was first coined in 1955 at the Dartmouth Workshop [1]. \\nJohn McCarthy, one of the key contributors at the conference \\nand an AI pioneer, defined AI as \"the science and engineering \\nof making intelligent machines, especially intelligent computer \\nprograms. It is related to the similar task of using computers to \\nunderstand human intelligence, but AI does not have to confine \\nitself to methods that are biologically observable\" [2].  \\nAI has gone through multiple cycles of boom and bust. \\nIt was a rising star from its inception to 1973. Scientists were \\nexcited by its potential to solve algebra word problems, prove \\ngeometry theorems, and even learn to speak. However, it failed \\nto deliver on the hyped expectations. That led to the Lighthill \\nReport in 1974, which triggered a massive loss of confidence in \\nAI [3]. In the United States, the Defense Advanced Research \\nProjects Agency (DARPA) also drastically reduced its AI \\nfunding. AI sank into an “AI Winter\" until 1980. The relief in \\nthe 80s turned out to be short-lived. In 1987, AI was again placed \\nin a freezer. By the early 2000s, AI was haunted by over-\\npromises and under-delivery. In 2005, John Markoff in the New \\nYork Times described that some computer scientists avoided the \\nterm artificial intelligence altogether “for fear of being viewed \\nas wild-eyed dreamers\" [4]. In 2007, Alex Castro referred to \\nartificial intelligence as a subject that has “too often failed to live \\nup to their promises\" [5]. That has resulted in “once something \\nbecomes useful enough and common enough it’s not labeled AI \\nanymore\" [6]. In the 1990s and 2000s, new computer science \\ndisciplines flourished. However, they were deliberately not \\ncategorized under Artificial Intelligence, for example, \\ninformatics, machine learning, machine perception, analytics, \\npredictive analytics, decision support systems, knowledge-\\nbased systems, business rules management, cognitive systems, \\nintelligent systems, language models, intelligent agents, or \\ncomputational intelligence. \\nAI regained its popularity with Google Translate, \\nGoogle Image Search, and IBM Watson’s winning the \\nJeopardy game in 2011 [7]. 2012 marked a turning point for AI \\ndue to breakthroughs in deep learning and GPU technology. \\nAlexNet used GPU to train its Convolutional Neural Network \\n(CNN) model to recognize and label images automatically and \\nwon the ImageNet 2012 Challenge by a large margin[8] [9]. \\nAI’s resurgence became insurmountable. AI broadened its \\nscope to absorb many downstream research fields. It became \\nan aggregator and a destination.  \\nIn November 2022, Open AI released ChatGPT which \\nattracted intense interest from the general public. It triggered an \\nall-out war between the Big Techs and stiffened competition \\nbetween rival countries. Ethical AI development and \\ndeployment have become more important than ever.  \\n \\nIII. METHODS \\n \\nWe selected SCOPUS1 as the main data source and \\nVOSviewer2 as the data aggregator. In the SCOPUS database, \\nwe searched for “AI ethics\" OR “artificial intelligence ethics\" \\nOR “machine ethics\" OR “algorithm ethics\" OR “information \\nethics\" OR \"ethics of technology\" OR “Robotic Ethics\" OR \\n“Robot Ethics\" OR “artificial moral agent\" OR “artificial moral \\nagents\" from 2004 to 2023, a period of 20 years, for all \\nlanguages, all countries, and territories. In total, 2,517 articles \\nwere selected. After removing 60 entries due to missing info, a \\ntotal of 2,457 pieces of literature were included in this analysis. \\nFor keyword analysis, we used 2004-2023 co-occurrence author \\nkeywords from VOSviewer. We used “Full counting\", which \\nmeans each keyword is counted as one regardless of how many \\nkeywords were listed in the literature. We then exported the \\nresults for time series and pattern analysis.  \\nIV. AI ETHICS DEVELOPMENT  \\nA. AI ethics and related ethics usage analysis \\nWe calculated the keyword usage frequencies for AI \\nEthics and other related ethical fields based on SCOPUS data. \\nOther related ethical fields included information ethics, machine \\nethics, roboethics, technology ethics, computer ethics, data \\nethics, engineering ethics, digital ethics, and computational \\nethics. The data is summarized in Figure 1.  \\n\"AI Ethics\" or \"Artificial Intelligence ethics\" first \\nappeared in keywords in 2008, followed by five 5 years of \\nhibernation. In 2014, AI Ethics reemerged and has since enjoyed \\nexponential growth. In 2014, there was only one occurrence of \\nthe keyword “AI Ethics” in our literature research. However, by \\n2022, the keyword frequency had increased to 148. In the 2023 \\npartial year till July 28, the usage has also reached 114. The \\nkeyword “AI Ethics” completely outnumbered the rest of the \\nterms such as “roboethics”, “data ethics”, or “machine ethics”.  \\nThis finding is important because in the historical \\ndevelopment section of AI, we know that 2012 marked the \\nturning point for AI as a research field. We believe 2014 is the \\nyear that AI Ethics was formed. Before that, AI Ethics was \\ndispersed across information ethics, machine ethics, roboethics, \\ntechnology ethics, and computer ethics. Thus, we define the pre-\\n2014 period as the incubation period.   \\n \\nB. Ethics principles usage pattern analysis \\nWe leveraged VOSviewer to unpack keyword usages \\nthat are related to AI Ethics. Figure 2 is generated from the \\nVOSviewer based on data since 2004 by using co-occurrence \\ndata and author keywords. We used full counting instead of \\npartial counting, which means each author keyword is counted \\nas one regardless of how many keywords were used in the \\nliterature. The size of the circle indicates the keywords’ relative \\nfrequency. Color represents the closeness of the topics. \\nWe sorted this information chronologically and further \\nbifurcated the keywords based on product orientation and their \\nassociations with AI Ethics principles. The information is \\npresented in Figure 3. The top rectangles are product-oriented \\n \\n1 www.scopus.com \\nfeatures, the middle ovals illustrate when keywords started to be \\nconsistently used. \\nThe results revealed a significant shift in AI Ethics \\nresearch principles from 2020. Between 2014 and 2019, AI \\nEthics keywords focused on principles to make AI ethical \\nhumans, e.g., trust, empathy, justice, care, and fairness. From \\n2020, however, the AI Ethics keywords were increasingly \\nfocused on protecting the downside risks and making AI \\nexplainable, accountable, trustworthy, non-biased, non-\\ndiscriminatory, less opaque, and for-diversity. \\nBased on this observation, we grouped AI Ethics \\ndevelopment into the following three phases: \\n \\n• \\nPhase I: Incubation (2004 to 2013) \\n• \\nPhase II: Making AI human-like machines (2014 to 2019)  \\n• \\nPhase III: Making AI human-centric machines (2020 and \\non) \\n \\nC. AI Ethics Development Phases \\nIn the following section, we will go through each \\nphase and highlight the major developments. \\n1) Phase I: Incubation (2004-2013)  \\nAI Ethics trailed AI’s rapid development. In 1997, \\nIBM’s Deep Blue defeated the world chess champion Garry \\nKasparov [10]. In 2005, the Stanford autonomous vehicle, \\nStanley, successfully crossed 212 kilometers of terrain in the \\nMojave Desert [11]. In 2011, IBM’s Watson won the Jeopardy! \\nthat conventional wisdom believed only humans could master \\n[12]. AI technology’s fast development galvanized many \\nexciting research fronts and, in a way, “pushed” AI Ethics to \\nthe forefront. It became apparent that other ethics would not be \\nsufficient to cover the wide spectrum of fields that AI covers. \\nAI Ethics as a research field was born.  \\nThe popular keyword in 2004 was “privacy\". Privacy \\nwas not a new discipline; neither was it exclusive to AI. It \\nbecame a hot topic with the explosive growth of data collection \\nand data usage in the Internet age. In the next few years, \\n“autonomy\", \\n“reliability\", \\n“safety\", \\n“security\", \\nand \\n“sustainability\" surfaced. The majority of the keywords were \\nproduct-oriented. \\n2) Phase II: Make AI Human-like Machines (2014-2019)  \\nIn Phase II, AI has increasingly demonstrated its \\npotential to function like a human. AI Ethicists and the general \\npublic welcomed the development. AI Ethics’ focus was on the \\nethical application of AI, the mini-human, in different fields. \\nFor this reason, we labeled this phase “Make AI Human-like \\nMachines\".  \\nDuring this phase, AI continued its breakneck \\nadvancement and pushed deep into new frontiers. In 2014, \\ngenerative adversarial networks (GAN) were developed to \\nsynthesize new and creative images from existing ones. In \\n2015, AI enabled machines to \"see\" and label images better than \\nhumans [13]. In 2016, Deep Mind’s AlphaGo defeated  \\n \\n \\n2 Vosviewer.com \\n \\n \\n \\nFig 1: Usage of different ethics in literature key words. AI Ethics first appeared in 2008. It consistently appeared after 2014 and have taken off since 2020.  \\n \\n \\n  \\n  \\nFig 2: AI Ethics and related fields based on bibliographical data in VOSviewer by using Scopus data from 2004 to July 28, 2023. \\n2\\n1\\n2\\n2\\n19 16\\n57\\n99\\n148\\n114\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\nUsage of different ethics in literature keywords \\n(Since 2004. 2023 partial year till 7/28)\\nai ethics\\ninformation ethics\\nmachine ethics\\nroboethics\\ntechnology ethics\\ncomputer ethics\\ndata ethics\\nengineering ethics\\ndigital ethics\\ncomputational ethics\\n  \\n \\nFig 3: AI Ethics Development Phases Based on Keyword Analysis \\n \\nworld Go champion Lee Sedol [14]. In 2018, AI beat human \\ndermatologists in accurately detecting skin cancer [15]. In \\n2018, Google Waymo’s Robotaxi started roaming in Phoenix’s \\nstreets [16]. The general public viewed the development \\npositively and was excited by AI’s boundless applications.  \\nHowever, AI Ethics development lagged behind AI \\ntechnology development. As mentioned in the historical \\nbackground section, AI regained popularity and broadened its \\nscope to include any computer science disciplines that enabled \\nhuman-like intelligence in 2012. Machine learning, machine \\nperception, text analysis, natural language processing (NLP), \\nlogical reasoning, game-playing, decision support systems, data \\nanalytics, and predictive analytics became AI’s upstream or \\nsupporting disciplines. Robotics (including autonomous \\nvehicles) was a fast-developing field that was enabled by AI. \\nAI became an aggregator.  \\nAI Ethics keywords during this phase reflected many \\nhuman-like features, for example, “accountability\", “care\", \\n“compassion\", \\n“empathy\", \\n“fairness\", \\n“justice\", \\n“transparency\", “trust\", and Explainable AI (XAI). The AI \\nEthics community wanted to make this intelligent technology \\naccountable, caring, compassionate, empathetic, fair, unbiased, \\ntransparent, and trustworthy, just like an ethical human.  \\n3) Phase III: Make AI Human-centric Machines (2020- \\npresent)  \\nIn Phase III, while continuing its rapid ascension, AI \\nhad shown aspects that were far from angelic. The AI Ethics \\ncommunity focused on grounding AI into an explainable, \\nresponsible, and trustworthy machine that serves humans \\ninstead of being a runaway alien technology. That was the \\nreason we titled this phase \"Make AI Human-centric \\nMachines\".  \\nBy 2020, AI had surpassed humans in handwriting \\nrecognition, speech recognition, image recognition, read \\ncomprehension, and language understanding [17]. In the \\nmeantime, Deep Fakes exacerbated online misinformation and \\nundermined basic human trust. In May 2021, the United States \\nNational Security Commission urged the US to win the AI arms \\nrace against China, reminiscing the costly and dangerous Cold \\nWar [18]. In July 2022, Google fired an engineer who claimed \\nthat its LaMDA language model was sentient, exacerbating the \\ngeneral public’s suspicion of AI [19]. In November 2022, Open \\nAI released Chat GPT 3.0 to the public and triggered an all-out \\nAI race. It looked increasingly like the AI companies were \\nracing to the bottom to win but put safety and security on the \\nbackburner [20]. In 2023, ChatGPT became the second Large \\nLanguage Model to pass the Turing Test [21] [22]. In 2023, \\nGoldman Sachs estimated that 300 million jobs could be \\ndisplacement by AI [23]. Scientists, entrepreneurs, and public \\nofficers started to alarm the general public about the \\nconsequences of unconstrained AI development. Public distrust \\nof AI surged.  \\nDuring this phase, precautionary keywords showed up \\nvery frequently in the literature, such as “algorithm bias\", “AI \\nbias\", “gender bias\", and “discrimination\" in 2020, and \\n“opacity\", “responsibility gap\", and “social justice\" in 2021. \\nMeanwhile, \\nkeywords \\nsuch \\nas \\n“explainability\" \\nand \\n“trustworthy AI’ popped up in 2020. “Human-centric\", \\n“responsible AI\" showed up in 2021, and “interpretability\" and \\n“sustainable AI\" showed up in 2022. The AI ethics community \\nwanted to make AI responsible, explainable, and trustworthy to \\nhumans. AI Ethics entered a phase to make AI \"human-centric\". \\nD. The future of AI Ethics   \\nAI technology is disruptive in nature. AI Ethics is \\npivotal in the benign and benevolent rollout of AI technology. \\nWith the current development pace, it is almost inevitable that \\nAI and robotics will match or surpass humans both physically \\nand intellectually. AI can become near-human. AI ethicists may \\nneed to explore how to make these intelligent AI embodiments \\n“machine-like humans”: machines that are intelligent and \\ncapable, but never achieve the same status as full humans. \\nAnother challenge is the confluence of AI, robotics, \\nand biotechnology. Machine-augmented humans and machine-\\naugmented non-human (animals or newly created species) \\ncould blur the definition of humans. AI ethicists may need to \\nstudy the ethical ramifications of this development and draw \\nredlines on socially acceptable creations of alien beings.  \\nSuperintelligence, a form of AI that is superior to \\nhuman intelligence, can be a concern. Even if the risks are \\nextremely low, if indeed it happens, the consequences could be \\nincredibly serious. AI Ethicists may need to develop a \\nframework to prevent Superintelligence from remotely \\nhappening. \\nV. LIMITATION \\nOur bibliometric analysis has a few limitations:  \\n1. We rely heavily on SCOPUS for data sources. Literature \\nunlisted on SCOPUS would be excluded. \\n2. We relied heavily on VOSviewer to generate keyword co-\\noccurrence data. Errors in VOSviewer could be carried \\ninto our final analysis.  \\n3. Literature search was based on literature titles and \\nkeywords. That could result in the addition of unwanted \\npapers. Some legitimate AI ethics articles might be \\nexcluded.   \\n4. The majority of literature included in this study was in \\nEnglish (97%). It is highly probable that some non-\\nEnglish literature was missed.  \\nDespite these caveats, we believe that our bibliometric analysis \\nremains highly valuable to the scientific and engineering \\ncommunity since most AI and AI Ethics research is published \\nin English-language journals and conferences that are indexed \\nby Scopus. \\nVI. CONCLUSION \\nThe bibliometric analysis of AI Ethics literature has \\npointed to a 3-phase AI Ethics development, namely incubation, \\nmaking AI human-like machines, and making AI human-centric \\nmachines. AI ethicists may need to get ahead of the AI \\ntechnology development and research on making AI machine-\\nlike humans, prohibit unethical development of machine-\\naugmented non-humans, and prevent the development of \\nmalicious or malevolent Superintelligence.  \\nVII. ACKNOWLEDGEMENT \\nThe work reported herein was supported by the National \\nScience Foundation (NSF) (Award #2246920). Any opinions, \\nfindings, and conclusions or recommendations expressed in \\nthis material are those of the authors and do not necessarily \\nreflect the views of the NSF. \\n \\n \\n[1] \\nDartmouth University, “Dartmouth workshop,” 1956. \\nhttps://home.dartmouth.edu/about/artificial-intelligence-ai-coined-\\ndartmouth \\n[2] \\nJohn McCarthy, “What is ai?,” 2023. http://jmc.stanford.edu/artificial-\\nintelligence/what-is-ai/index.html \\n[3] \\nJames Lighthill, “Lighthill report: Artificial intelligence: a paper \\nsymposium,” Science Research Council, London, 1973. \\n[4] \\nJohn Markoff, “Behind artificial intelligence, a squadron of bright real \\npeople,” 2005. New York Times. \\n[5] \\nPatty Tascarella, “Are you talking to me?” 2005. \\nhttps://www.economist.com/technology-quarterly/2007/06/09/are-you-\\ntalking-to-me \\n[6] \\nCNN, “Ai set to exceed human brain power,” 2006. CNN.com.  \\n[7] \\nMike Hale, “Actors and their roles for $300, hal? hal!” \\nNew York Times, 2011 \\n[8] \\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, “Imagenet \\nclassification with deep convolutional neural networks,” \\nCommunications of the ACM, vol. 60, 2017. \\n[9] \\nDave Gershgorn, “The inside story of how ai got good \\nenough to dominate Silicon Valley,” Quartz, 2018. \\n[10] Bruce Weber, “Swift and slashing, computer topples \\nkasparov,” 1997. https://www.nytimes.com/1997/05/12/nyregion/swift-\\nand-slashing-computer-topples-kasparov.html \\n[11] Steve Russell, “Darpa grand challenge winner: Stanley the robot!” \\n2006. \\nhttps://www.popularmechanics.com/technology/robots/a393/2169012/ \\n[12] John Markoff, “Computer wins on ‘Jeopardy!’: Trivial, \\nit’s not!,” 2011. \\nhttps://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html \\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, \\n“Imagenet: A large-scale hierarchical image database,” in 2009 IEEE \\nConference on Computer Vision and Pattern Recognition, 2009, pp. \\n248–255. \\n[14] Adrian Cho, “‘Huge leap forward’: Computer that mimics human brain \\nbeats professional at game of go,” Science, 2016. \\n[15] Alexa Lardieri, “Ai beats doctors at cancer diagnoses,” \\n2018. https://www.usnews.com/news/health-care-news/articles/2018-05-\\n28/artificial-intelligence-beats-dermatologists-at-diagnosing-skin-cancer \\n[16] Jon Fingas, “Waymo launches its first commercial self-driving car \\nservice,” 2019. \\nhttps://en.wikipedia.org/wiki/Waymo#:~:text=In%20December%20201\\n8%2C%20Waymo%20launched,and%20request%20a%20pick%2Dup \\n[17] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus \\nGeiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet \\nSingh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, \\nZeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, \\nChristopher Potts, and Adina Williams, “Dynabench: Rethinking \\nbenchmarking in nlp,” 2021. \\n[18] National Security Commission on Artificial Intelligence, “National \\nsecurity commission on Artificial Intelligence - final report,” 2021. \\nhttps://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-\\n1.pdf \\n[19] Miles Kruppa, “Google parts with engineer who claimed its ai system is \\nsentient,” 2022. https://www.wsj.com/articles/google-parts-with -\\nengineer-who-claimed-its-ai-system-is-sentient-\\n11658538296?ns=prod/accounts-wsj \\n[20] Samantha Lock, “What is ai chatbot phenomenon chat-gpt and could it \\nreplace humans?,” 2022, Accessed: 2022-12-05. \\nhttps://www.theguardian.com/technology/2022/dec/05/what-is-ai-\\nchatbot-phenomenon-chatgpt-and-could-it-replace-humans \\n[21] Celeste Biever, “Chatgpt broke the Turing test - the race \\nis on for new ways to assess ai,” 2023. Nature, vol. 619, 2023 \\n[22] Will Oremus, “Google’s ai passed a famous test — and showed how the \\ntest is broken,” 2022. The Washington Post.  \\n[23] Beatrice Nolan, “AI systems like chatgpt could impact 300 million full- \\ntime jobs worldwide, with administrative and legal roles \\nsome of the most at risk, goldman sachs report says,” \\nhttps://www.businessinsider.com/generative-ai-chatpgt-300-million-full-\\ntime-jobs-goldman-sachs-2023-3.\\n \\n'), Document(metadata={'Published': '2024-05-07', 'Title': 'Interaction Design for Human-AI Choreography Co-creation', 'Authors': 'Yimeng Liu', 'Summary': \"Human-AI co-creation aims to combine human and AI strengths for artistic\\nresults exceeding individual capabilities. Frameworks exist for painting,\\nmusic, and poetry, but choreography's embodied nature demands a dedicated\\napproach. This paper explores AI-assisted choreography techniques (e.g.,\\ngenerative ideation, embodied improvisation) and analyzes interaction design --\\nhow humans and AI collaborate and communicate -- to inform the design\\nconsiderations of future human-AI choreography co-creation systems.\"}, page_content='Interaction Design for Human-AI Choreography Co-creation\\nYIMENG LIU, University of California, Santa Barbara, USA\\nHuman-AI co-creation aims to combine human and AI strengths for artistic results exceeding individual capabilities. Frameworks exist\\nfor painting, music, and poetry, but choreography’s embodied nature demands a dedicated approach. This paper explores AI-assisted\\nchoreography techniques (e.g., generative ideation, embodied improvisation) and analyzes interaction design — how humans and AI\\ncollaborate and communicate — to inform the design considerations of future human-AI choreography co-creation systems.\\nCCS Concepts: • Human-centered computing →Interactive systems and tools.\\nAdditional Key Words and Phrases: human-AI collaboration, choreography creation, creativity support\\n1\\nINTRODUCTION\\nHuman-AI co-creativity is a collaborative process where humans and AI work together as partners to create innovative\\nsolutions, artistic works, or other creative outputs. This process depends heavily on the interaction dynamics, roles of\\neach participant, and communication styles employed. Careful design of these elements is essential for maximizing\\nthe effectiveness and benefits of human-AI co-creative systems, such as increased efficiency and enhanced creativity.\\nBuilding upon an established interaction design framework for human-AI co-creativity [17] across domains like painting,\\nmusic, storytelling, and poetry [13, 14, 18, 20], this paper focuses on a relatively unexplored domain: choreography\\nco-creation with AI. This inherently embodied and highly creative research field needs tailored interaction design\\ninsights to unlock its full potential. In this work, we present existing AI-supported choreography systems and techniques\\nand analyze their interaction design through the lens of three distinct design goals: choreography generation, creativity\\nsupport, and human-AI choreography co-creation. Inspired by the computational creativity research by Davis et al. [5], we\\ncategorize existing systems based on these goals and uncover three key interaction design considerations: facilitating\\nparallel and spontaneous interaction between humans and AI, assigning distinct yet complementary roles to humans\\nand AI, and ensuring effective human-AI communication. These insights aim to serve as a resource for designing future\\nsystems and refining existing ones, ultimately pushing the boundaries of human-AI co-creation in choreography.\\n2\\nRELATED WORK\\n2.1\\nChoreography Generation\\nPrevious research on AI-assisted choreography generation has primarily aimed at developing techniques for automat-\\nically creating innovative, unexpected, and valuable dance concepts and materials. Much of this work has explored\\nusing generative AI models, such as diffusion models, to facilitate this process. These models have taken diverse input\\nmodalities like music, text, and video, transforming extracted features into dance movements [1, 2, 6, 19, 22, 23, 25].\\n2.2\\nCreativity Support\\nPrior work on AI-based creativity support has utilized techniques like tracking history, simulating possibilities, and\\nexploring alternatives to assist individuals in their creative endeavors. For instance, systems like [4, 11, 15] have\\nleveraged generative AI to augment creativity during choreography ideation. These systems have empowered users to\\ngenerate new movements, iteratively edit dance sequences, document creative practice, and foster the exploration of\\nboth system-generated and user-provided ideas, thereby supporting user creative potential.\\nLicensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Copyright remains with the author(s).\\nGenAICHI: CHI 2024 Workshop on Generative AI and HCI\\n1\\narXiv:2405.03999v1  [cs.HC]  7 May 2024\\nInteraction Design for Human-AI Choreography Co-creation\\nYimeng Liu\\n2.3\\nHuman-AI Choreography Co-creation\\nResearch on human-AI choreography co-creation has worked on developing co-creative agents that engage in real-\\ntime improvisation with humans to enrich the creative process. Systems like Viewpoints AI [8], LuminAI [12], and\\nRobodanza [7] have fostered collaborative engagement during choreography. These systems have enabled humans and\\nAI to take spontaneous initiatives, contributing jointly to the creation of dance movements.\\n3\\nINTERACTION DESIGN FOR AI-SUPPORTED CHOREOGRAPHY CREATION\\nBuilding on Ciolfi et al.’s four-stage choreography creation process [3]: preparation, studio, performance, and reflection,\\nwe focus on the first two stages where AI can shine. Leveraging its content generation capabilities, AI can empower\\nchoreographers during ideation and prototyping. Drawing on relevant research for each stage, we analyze human-AI\\ninteraction through the lens of Rezwania et al.’s co-creative framework for interaction design (COFI) [17], including the\\ncollaboration and communication styles between humans and AI.\\n3.1\\nInteraction Design for Choreography Ideation in the Preparation Stage\\nThe preparation stage focuses on ideation and crafting choreographic materials. However, research on human-AI\\nco-creation for this stage remains scarce, so we discuss techniques developed for choreography generation and creativity\\nsupport and explore how these techniques can be expanded to foster co-creativity regarding interaction design.\\n3.1.1\\nCollaboration Style. Most previous research has adopted a turn-taking collaboration style in the preparation\\nstage, where humans and AI alternate to contribute to the same or separate tasks. In the same-task scenario, humans\\nutilize AI-based techniques to generate artifacts with convergent or divergent ideas. For example, existing work [4, 6, 11]\\nallows both humans and AI to contribute to the same dance sequences. The underlying generative AI model is called\\nupon when humans initiate dance generation or modification. Conversely, AI-based methods can potentially support\\nthe evaluation of created artifacts or user-provided concepts in divided tasks. This branch has not been fully explored in\\nprior work. However, leveraging effective human motion evaluation techniques, such as Laban Movement Analysis [10],\\ncan enhance the understanding of abstract movements and contribute to choreography creation. Regarding the timing\\nof initiative, AI typically responds to human requests when ideation or evaluation is needed, as prior research has\\nshown that users tend to be opposed to AI taking the lead in turn-taking interaction [21].\\n3.1.2\\nCommunication Style. During the preparation stage, where intense brainstorming is key, an on-demand interac-\\ntion design is necessary to balance creative thinking with the absorption of new information. To achieve this, clear\\nand direct communication between humans and AI is important. Human-to-AI communication can leverage intuitive\\nmethods like text, voice, and direct manipulation. These methods can facilitate the seamless transmission of needs and\\ncreative vision, as demonstrated by choreographers who utilize them to communicate their ideas effectively [3, 24].\\nAI-to-human communication can rely on easily understandable text and visuals to present dance poses or sequences, as\\nwell as evaluation results for movements.\\nWhile intentional communication plays a key role, few efforts have explored the potential of consequential commu-\\nnication in human-AI interaction. This approach complements intentional communication when direct conversation\\nfails to capture a nuanced creative vision. For example, choreographers often struggle to articulate implicit feelings in\\ndance ideas through words alone [17]. They may rely on sound, facial expressions, or gestures to convey these nuances.\\nThis presents a challenge for AI in processing such subtle information and offering relevant choreographic materials\\nGenAICHI: CHI 2024 Workshop on Generative AI and HCI\\n2\\nInteraction Design for Human-AI Choreography Co-creation\\nYimeng Liu\\nthat align with the artist’s intent. Furthermore, research on mixed-initiative human-AI communication for co-creativity\\nis limited. Understanding how the level of interaction, e.g., reactive vs. proactive AI, impacts human-AI communication\\nin the preparation stage remains an open question.\\n3.2\\nInteraction Design for Choreography Prototyping in the Studio Stage\\nShifting gears to the studio stage, the focus is on translating ideas into movement and collaborating with other dancers\\nand choreographers. Here, embodiment becomes essential in interaction design. By analyzing the interaction design of\\nexisting co-creativity systems in this stage, we uncover current challenges and pose open research questions.\\n3.2.1\\nCollaboration Style. Existing research has explored parallel collaboration styles, where humans and AI share\\nmixed initiatives to contribute to a shared task. Examples include Viewpoints AI [8], LuminAI [12], and Robodanza [7],\\nall designed to facilitate real-time, collaborative dance improvisation and performance. These systems have enabled AI\\nto capture and process human motion and generate new dance movements embodied by projections or robots that\\ncomplement or react to the human dancers. Importantly, initiative timing is spontaneous, with humans and AI free to\\ninitiate and modify dance poses and movements, contributing to the evolving artifact.\\n3.2.2\\nCommunication Style. For an unobtrusive and immersive experience in the studio stage, interaction design\\nrequires mirroring human communication styles through both explicit and implicit methods. Human-to-AI communica-\\ntion can utilize intentional methods like voice and direct manipulation alongside consequential methods like facial\\nexpressions and embodied cues. This aligns with how humans naturally communicate, offering a broader spectrum of\\ninformation exchange. AI, on the other hand, can utilize speech, haptics, and visuals to respond.\\nPrevious research often overlooks the design of human-to-AI consequential and AI-to-human communication in the\\nstudio stage despite their crucial role in fostering embodied experiences. Just like humans observing others to understand\\ntheir movement and intent, AI needs to develop a similar Theory of Mind [16] to interpret human mental states beyond\\nexplicit instructions. In the studio stage, where information exchange is frequent and initiative is spontaneous, relying\\nsolely on explicit communication hinders AI’s effectiveness as a collaborator and communicator. Consequently, AI\\nsystems need to be proactive and sensitive to implicit information to achieve true collaboration.\\n4\\nDISCUSSION AND FUTURE DIRECTIONS\\nTable 1. Design and interaction of choreography-support systems in the choreography preparation and studio stages.\\nType\\nPaper\\nDescription\\nPreparation Stage\\nStudio Stage\\nCollaboration\\nCommunication\\nCollaboration\\nCommunication\\nChoreography Gen-\\neration\\n[1, 2, 6, 19,\\n22, 23, 25]\\nConvert multimodal input\\ninto dance motion\\nTurn-taking,\\nshared\\ntask,\\nreactive\\nHuman–>AI: Intentional\\nAI–>Human: Intentional\\nCreativity Support\\n[4, 11, 15]\\nAugment creativity via in-\\nteraction with the system\\nTurn-taking,\\nshared\\ntask,\\nreactive\\nHuman–>AI: Intentional\\nAI–>Human: Intentional\\nHuman-AI Choreog-\\nraphy Co-creation\\n[7, 8, 12]\\nCo-create dance based on\\ncollaborative engagement\\nParallel,\\nshared\\ntask,\\nproactive\\nHuman–>AI: Intentional\\nAI–>Human: N/A\\nGenAICHI: CHI 2024 Workshop on Generative AI and HCI\\n3\\nInteraction Design for Human-AI Choreography Co-creation\\nYimeng Liu\\nFig. 1. Interaction design space for AI-supported choreogra-\\nphy creation. The three axes are built upon the co-creative\\nframework for interaction design introduced in [17].\\nTable 1 summarizes the discussed research, comparing their\\ndesign and interaction approaches covered in Sections 2 and 3.\\nThis analysis yielded three key insights for designing future\\nhuman-AI choreography co-creation systems. We leverage\\nthese insights to explore the interaction design space in Figure 1,\\nwhich incorporates factors like participation style, task distri-\\nbution, and initiative timing (inspired by Rezwan et al. [17]).\\n4.1\\nBuilding Parallel and Spontaneous Collaboration\\nPrevious research focuses on parallel and spontaneous human-\\nAI collaboration in the studio stage, neglecting the potential for\\nAI to be a true partner throughout the entire process, including\\nin the preparation stage. Specifically, AI systems usually wait\\ntheir turn to assist in brainstorming and refining ideas, often\\nwhen humans request them. This turn-taking style positions\\nthem as tools rather than collaborators, as effective collaboration thrives on spontaneous exchange of feedback, which\\nis crucial for successful communication and task completion. Therefore, future research can focus on developing\\nAI that transcends simply waiting for its turn. By actively engaging in the creative process, these AI systems could\\nsignificantly enhance collaboration. Imagine AI that offers timely inspiration, provides constructive feedback, and\\nproposes refinements throughout choreography creation — acting as a concurrent source of creative input, independent\\nof human work at times. This shift would foster a more dynamic and collaborative experience.\\n4.2\\nDesigning Complementary Roles for Human and AI\\nExisting research on human-AI choreography collaboration often overlooks the crucial aspect of task division. While\\ncreating new choreography gets ample focus, tasks like expanding, refining, and transforming existing pieces remain\\nlargely unexplored. This gap might be due to current AI systems often mimicking user input or existing works, limiting\\ntheir ability to generate truly innovative and thought-provoking pieces. However, such outputs can be instrumental\\nin sparking divergent thinking, a technique proven to enhance creativity [11]. In essence, these AI-generated pieces\\ncould ignite a deeper exploration of creative concepts and materials, offering a wider range of possibilities to build\\nupon existing choreography [9].\\n4.3\\nEnabling Effective Dialogue and Mutual Understanding\\nThe reviewed papers highlight a gap in effective human-AI communication during choreography creation. In the\\npreparation stage, the interaction leans heavily towards a one-way flow. Humans initiate ideas and instructions, while\\nthe AI passively responds. The dynamic improves somewhat during the studio stage, where both humans and AI\\ncontribute elements to the dance piece. However, achieving direct communication from AI to humans similar to\\nhuman-to-human interaction remains challenging. Moving forward, research can explore the potential of proactive\\nAI communication styles. Imagine an AI system that actively monitors a dancer’s movements and offers constructive\\nsuggestions based on its observation. This would mirror the dynamic of a human collaborator, fostering a richer creative\\nprocess. Furthermore, integrating consequential communication from humans to AI is crucial to fostering a more natural\\nand immersive co-creative experience. This is especially important in the highly embodied realm of choreography.\\nGenAICHI: CHI 2024 Workshop on Generative AI and HCI\\n4\\nInteraction Design for Human-AI Choreography Co-creation\\nYimeng Liu\\nREFERENCES\\n[1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. 2023. Listen, Denoise, Action! Audio-Driven Motion Synthesis with\\nDiffusion Models. ACM Trans. Graph. 42, 4, Article 44 (jul 2023), 20 pages. https://doi.org/10.1145/3592458\\n[2] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. 2019. Everybody dance now. In Proceedings of the IEEE/CVF international conference\\non computer vision. IEEE, New York, NY, USA, 5933–5942.\\n[3] Marianela Ciolfi Felice, Sarah Fdili Alaoui, and Wendy E. Mackay. 2016. How Do Choreographers Craft Dance? Designing for a Choreographer-\\nTechnology Partnership. In Proceedings of the 3rd International Symposium on Movement and Computing (Thessaloniki, GA, Greece) (MOCO ’16).\\nACM, New York, NY, USA, Article 20, 8 pages. https://doi.org/10.1145/2948910.2948941\\n[4] Luka Crnkovic-Friis and Louise Crnkovic-Friis. 2016. Generative Choreography using Deep Learning. arXiv:1605.06921 [cs.AI]\\n[5] Nicholas Davis, Chih-Pin Hsiao, Yanna Popova, and Brian Magerko. 2015. An Enactive Model of Creativity for Computational Collaboration and\\nCo-creation. Springer London, London, 109–133. https://doi.org/10.1007/978-1-4471-6681-8_7\\n[6] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang. 2023. Tm2d: Bimodality driven\\n3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision. IEEE, New York, NY,\\nUSA, 9942–9952.\\n[7] I Infantino, A Augello, A Manfré, G Pilato, and F Vella. 2016. Robodanza: Live performances of a creative dancing humanoid. In Proceedings of the\\nSeventh International Conference on Computational Creativity. ACC, Coimbra, Portugal, 388–395.\\n[8] Mikhail Jacob, Alexander Zook, and Brian Magerko. 2013. Viewpoints AI: Procedurally Representing and Reasoning about Gestures.. In DiGRA\\nconference. DiGRA conference, Finland, 1–15.\\n[9] Anna Kantosalo and Hannu Toivonen. 2016. Modes for creative human-computer collaboration: Alternating and task-divided co-creativity. In\\nProceedings of the Seventh International Conference on Computational Creativity. ACC, Coimbra, Portugal, 77–84.\\n[10] Rudolf von Laban. 1975. Modern educational dance / by Rudolf Laban. (3rd ed. / revised with additions by lisa ullmann. ed.). Macdonald and Evans,\\nBraintree, MA, USA.\\n[11] Yimeng Liu and Misha Sra. 2024. Exploring AI-assisted Ideation and Prototyping for Choreography. In Companion Proceedings of the 29th\\nInternational Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI ’24 Companion). ACM, New York, NY, USA, 11–17.\\nhttps:\\n//doi.org/10.1145/3640544.3645227\\n[12] Duri Long, Mikhail Jacob, Nicholas Davis, and Brian Magerko. 2017. Designing for socially interactive systems. In Proceedings of the 2017 ACM\\nSIGCHI Conference on Creativity and Cognition. ACM, New York, NY, USA, 39–50.\\n[13] Alex Rodriguez Lopez, Antonio Pedro Oliveira, and Amílcar Cardoso. 2010. Real-Time Emotion-Driven Music Engine.. In ICCC. IEEE, New York,\\nNY, USA, 150–154.\\n[14] Hiroyuki Osone, Jun-Li Lu, and Yoichi Ochiai. 2021. BunCho: ai supported story co-creation via unsupervised multitask learning to increase writers’\\ncreativity in japanese. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA, 1–10.\\n[15] Mariel Pettee, Chase Shimmin, Douglas Duhaime, and Ilya Vidrin. 2019. Beyond Imitation: Generative and Variational Choreography via Machine\\nLearning. arXiv:1907.05297 [cs.LG]\\n[16] David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences 1, 4 (1978), 515–526.\\n[17] Jeba Rezwana and Mary Lou Maher. 2023. Designing creative AI partners with COFI: A framework for modeling interaction in human-AI co-creative\\nsystems. ACM Transactions on Computer-Human Interaction 30, 5 (2023), 1–28.\\n[18] Angie Spoto, Natalia Oleynik, Sebastian Deterding, and Jon Hook. 2024. Library of Mixed-Initiative Creative Interfaces. Digital Creativity Lab,\\nUniversity of York. http://mici.codingconduct.cc/\\n[19] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. 2023. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition. IEEE, New York, NY, USA, 448–458.\\n[20] Tom White and Ian Loh. 2017. Generating Animations by Sketching in Conceptual Space.. In ICCC. IEEE, New York, NY, USA, 261–268.\\n[21] Lauren Winston and Brian Magerko. 2017. Turn-taking with improvisational co-creative agents. In Proceedings of the Thirteenth AAAI Conference\\non Artificial Intelligence and Interactive Digital Entertainment (Little Cottonwood Canyon, Utah, USA) (AIIDE’17). AAAI Press, Little Cottonwood\\nCanyon, Utah, USA, Article 19, 8 pages.\\n[22] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, and Yanfeng Wang. 2020. ChoreoNet: Towards Music to Dance Synthesis with\\nChoreographic Action Unit. In Proceedings of the 28th ACM International Conference on Multimedia (Seattle, WA, USA) (MM ’20). ACM, New York,\\nNY, USA, 744–752. https://doi.org/10.1145/3394171.3414005\\n[23] Mingao Zhang, Changhong Liu, Yong Chen, Zhenchun Lei, and Mingwen Wang. 2022. Music-to-Dance Generation with Multiple Conformer.\\nIn Proceedings of the 2022 International Conference on Multimedia Retrieval (Newark, NJ, USA) (ICMR ’22). ACM, New York, NY, USA, 34–38.\\nhttps://doi.org/10.1145/3512527.3531430\\n[24] Qiushi Zhou, Cheng Cheng Chua, Jarrod Knibbe, Jorge Goncalves, and Eduardo Velloso. 2021. Dance and choreography in HCI: a two-decade\\nretrospective. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA, 1–14.\\n[25] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia. 2022. Music2Dance: DanceNet for Music-Driven Dance\\nGeneration. ACM Trans. Multimedia Comput. Commun. Appl. 18, 2, Article 65 (feb 2022), 21 pages. https://doi.org/10.1145/3485664\\nGenAICHI: CHI 2024 Workshop on Generative AI and HCI\\n5\\n')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs=ArxivLoader(query=\"Generative AI\",load_max_docs=2).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Environment",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
